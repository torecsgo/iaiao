# ğŸ“˜ RAG Penal EspaÃ±ol

Este proyecto implementa un sistema de **Retrieval-Augmented Generation (RAG)** utilizando el **CÃ³digo Penal EspaÃ±ol** como corpus base. El objetivo es responder preguntas del usuario Ãºnicamente con informaciÃ³n contenida en dicho corpus, sin alucinaciones o invenciones.

## ğŸš€ TecnologÃ­as usadas

- [`sentence-transformers`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) para generar embeddings semÃ¡nticos.
- [`PyMuPDF`](https://pymupdf.readthedocs.io/) (`fitz`) para leer el PDF del CÃ³digo Penal.
- [`Ollama`](https://ollama.com) como backend LLM (probado con `llama3.2`).
- Python 3.9+

## ğŸ“‚ Estructura del proyecto

```
â”œâ”€â”€ RAG.py             # Clase para dividir el corpus y recuperar chunks relevantes
â”œâ”€â”€ Ollama.py          # Clase HTTP para interactuar con Ollama
â”œâ”€â”€ iaiao.ipynb        # Notebook de preguntas/respuestas
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ codigo_penal.pdf  # Corpus base (CÃ³digo Penal EspaÃ±ol)
```

## âš™ï¸ InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone https://github.com/tu-usuario/rag-penal-espanol.git
cd rag-penal-espanol
```

2. Instala las dependencias:
```bash
pip install sentence-transformers pymupdf requests numpy
```

3. AsegÃºrate de tener [Ollama](https://ollama.com) corriendo localmente y tener el modelo descargado:
```bash
ollama run llama3.2:3b
```

4. Coloca `codigo_penal.pdf` dentro de la carpeta `./docs/`

## ğŸ§  Â¿CÃ³mo funciona?

1. El corpus se divide en chunks solapados.
2. Se generan embeddings con `all-MiniLM-L6-v2`.
3. El usuario hace una pregunta â†’ se genera un embedding de la misma.
4. Se seleccionan los 3 chunks mÃ¡s similares al embedding de la pregunta.
5. Estos chunks se usan como contexto para generar una respuesta precisa con el modelo LLM.


## ğŸ›‘ Limitaciones

- Solo responde con base en el texto proporcionado.
- Si la informaciÃ³n no estÃ¡ en el corpus, el modelo indica que no puede responder.

## ğŸ“„ Licencia

MIT License